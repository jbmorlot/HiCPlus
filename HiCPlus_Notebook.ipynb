{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import gzip\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from time import gmtime, strftime\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "\n",
    "import h5py\n",
    "\n",
    "import src.model as models\n",
    "import src.utils as utils\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Constraint Pytoch to use the good GPU!\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX TITAN Black\n"
     ]
    }
   ],
   "source": [
    "print torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KeysSpec = ['ESC','CN'] #Keys Species\n",
    "data_path = {}\n",
    "data_path['ESC'] = '/home/invites/jmorlot/HDDTeam/data/Hi-C/Mouse/boyan/last_try2/ESC/'\n",
    "data_path['CN'] = '/home/invites/jmorlot/HDDTeam/data/Hi-C/Mouse/boyan/last_try2/CN/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# InputFiles ={key: [data_path[key]+f for f in os.listdir(data_path[key]) if re.search('basic',f)] for key in data_path.keys()} \n",
    "# OutputFiles ={key: [data_path[key]+f.replace('basic','HiCPlusBoost') for f in os.listdir(data_path[key]) if re.search('basic',f)] for key in data_path.keys()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESC\n",
      "\t0.1\n",
      "\t1.0\n",
      "\t10.0\n",
      "\t100.0\n",
      "CN\n",
      "\t0.1\n",
      "\t1.0\n",
      "\t10.0\n",
      "\t100.0\n"
     ]
    }
   ],
   "source": [
    "KeysSub = ['0.1','1.0','10.0','100.0'] #Keys Subsampling\n",
    "Data={}\n",
    "for keySp in KeysSpec:\n",
    "    print keySp\n",
    "    Data[keySp]={}\n",
    "    for keySu in KeysSub:\n",
    "        print '\\t' + keySu\n",
    "#         path = data_path[keySp] + 'chr16_basic_' + keySu + '.hdf5'\n",
    "        f = h5py.File(data_path[keySp] + 'chr16_basic_' + keySu + '.hdf5','r')\n",
    "        Data[keySp][keySu] = np.array(f[f.keys()[0]])\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESC\n",
      "\t0.1\n",
      "\t1\n",
      "\t10\n",
      "\t100\n",
      "CN\n",
      "\t0.1\n",
      "\t1\n",
      "\t10\n",
      "\t100\n"
     ]
    }
   ],
   "source": [
    "DataBoostHiC = {}\n",
    "for keySp in KeysSpec:\n",
    "    print keySp\n",
    "    DataBoostHiC[keySp]={}\n",
    "    for keySu in KeysSub:\n",
    "        print '\\t' + keySu\n",
    "#         path = data_path[keySp] + 'chr16_basic_' + keySu + '.hdf5'\n",
    "        f = h5py.File(data_path[keySp] + 'chr16_boosted_' + keySu + '.hdf5','r')\n",
    "        DataBoostHiC[keySp][keySu] = np.array(f[f.keys()[0]])\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiCPlus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_gpu = 1\n",
    "\n",
    "conv2d1_filters_numbers = 8\n",
    "conv2d1_filters_size = 9\n",
    "conv2d2_filters_numbers = 8\n",
    "conv2d2_filters_size = 1\n",
    "conv2d3_filters_numbers = 1\n",
    "conv2d3_filters_size = 5\n",
    "\n",
    "\n",
    "down_sample_ratio = 16\n",
    "epochs = 10\n",
    "HiC_max_value = 100\n",
    "\n",
    "max_range = 3000 # in paper=201\n",
    "batch_size = 32 # in paper = size of the dataset (silly...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewritting functions to fit with our matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide(HiCsample):\n",
    "    '''\n",
    "        Subdivide the HiC matrix in an ensemble of subimages of size subImage_size\n",
    "    '''\n",
    "    subImage_size = 40\n",
    "    step = 25\n",
    "    result = []\n",
    "    index = []\n",
    "    \n",
    "    total_loci = HiCsample.shape[0]\n",
    "    for i in range(0, total_loci, step):\n",
    "        for j in range(0, total_loci, ):\n",
    "            if (abs(i-j) > max_range or i + subImage_size >= total_loci or j + subImage_size >= total_loci):\n",
    "                continue\n",
    "            subImage = HiCsample[i:i+subImage_size, j:j+subImage_size]\n",
    "\n",
    "            result.append([subImage,])\n",
    "            index.append((i, j))\n",
    "\n",
    "    result = np.array(result)\n",
    "#     print result.shape\n",
    "    result = result.astype(np.double)\n",
    "    index = np.array(index)\n",
    "    return result, index\n",
    "\n",
    "def HiCPlus(HiCsample):\n",
    "    \n",
    "    ## Subdivide the HiC matrix in subimages\n",
    "    low_resolution_samples, index = divide(HiCsample)\n",
    "    \n",
    "    low_resolution_samples = np.minimum(HiC_max_value, low_resolution_samples)\n",
    "\n",
    "#     batch_size = low_resolution_samples.shape[0]\n",
    "    \n",
    "    ## Reshape the high-quality Hi-C sample as the target value of the training.\n",
    "    sample_size = low_resolution_samples.shape[-1]\n",
    "    padding = conv2d1_filters_size + conv2d2_filters_size + conv2d3_filters_size - 3\n",
    "    half_padding = padding / 2\n",
    "    output_length = sample_size - padding\n",
    "\n",
    "#     print low_resolution_samples.shape\n",
    "    \n",
    "    ## Data Loader\n",
    "    lowres_set = data.TensorDataset(torch.from_numpy(low_resolution_samples), torch.from_numpy(np.zeros(low_resolution_samples.shape[0])))\n",
    "    lowres_loader = torch.utils.data.DataLoader(lowres_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    hires_loader = lowres_loader\n",
    "    \n",
    "    ## Get Model\n",
    "    model = models.Net(40, 28)\n",
    "    model.load_state_dict(torch.load('model/pytorch_model_12000'))\n",
    "    if use_gpu:\n",
    "        model = model.cuda()\n",
    "\n",
    "    _loss = nn.MSELoss()\n",
    "    \n",
    "    ## Make predictions\n",
    "    running_loss = 0.0\n",
    "    running_loss_validate = 0.0 # WARNING : NOT USED\n",
    "    reg_loss = 0.0 # WARNING : NOT USED\n",
    "\n",
    "    y_prediction = []\n",
    "    for i, _lowRes in enumerate(lowres_loader):\n",
    "\n",
    "        _lowRes = Variable(_lowRes[0]).float()\n",
    "\n",
    "        _lowRes = _lowRes.cuda()\n",
    "        \n",
    "        _hiRes = model(_lowRes).data.cpu()\n",
    "        \n",
    "        y_prediction.append(_hiRes)\n",
    "        \n",
    "        del _lowRes,_hiRes\n",
    "\n",
    "    y_prediction = torch.cat(y_prediction)\n",
    "\n",
    "    y_predict = y_prediction.numpy()\n",
    "\n",
    "\n",
    "    print y_predict.shape\n",
    "    \n",
    "    ## Recombine samples\n",
    "    length = int(y_predict.shape[2])\n",
    "    y_predict = np.reshape(y_predict, (y_predict.shape[0], length, length))\n",
    "#     print y_predict.shape\n",
    "\n",
    "#     chrs_length = [249250621,243199373,198022430,191154276,180915260,171115067,159138663,146364022,141213431,135534747,135006516,133851895,115169878,107349540,102531392,90354753,81195210,78077248,59128983,63025520,48129895,51304566]\n",
    "\n",
    "#     chrN = 21\n",
    "\n",
    "    length = HiCsample.shape[0]\n",
    "\n",
    "    prediction_1 = np.zeros((length, length))\n",
    "    \n",
    "    for i in range(index.shape[0]):\n",
    "        x = int(index[i][0])\n",
    "        y = int(index[i][1])\n",
    "        prediction_1[x+6:x+34, y+6:y+34] = y_predict[i]\n",
    "        prediction_1[x+6:x+34, y+6:y+34] = y_predict[i]\n",
    "    \n",
    "    return prediction_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply HiCPlus to our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESC\n",
      "\t0.1\n",
      "(1798200, 1, 28, 28)\n",
      "\t1.0\n",
      "(1798200, 1, 28, 28)\n",
      "\t10.0\n",
      "(1798200, 1, 28, 28)\n",
      "\t100.0\n",
      "(1798200, 1, 28, 28)\n",
      "CN\n",
      "\t0.1\n",
      "(1792079, 1, 28, 28)\n",
      "\t1.0\n",
      "(1792079, 1, 28, 28)\n",
      "\t10.0\n",
      "(1792079, 1, 28, 28)\n",
      "\t100.0\n",
      "(1792079, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "DataBoosted={}\n",
    "for keySp in KeysSpec:\n",
    "    DataBoosted[keySp]={}\n",
    "    print keySp\n",
    "    for keySu in KeysSub:\n",
    "        print '\\t' + keySu\n",
    "        DataBoosted[keySp][keySu] = HiCPlus(Data[keySp][keySu])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keySp = 'ESC'\n",
    "# keySu = '1'\n",
    "figpath = 'Figures/'\n",
    "if not os.path.exists(figpath):\n",
    "    os.makedirs(figpath)\n",
    "for keySp in KeysSpec:\n",
    "    print keySp\n",
    "    for keySu in KeysSub:\n",
    "        print '\\t' + keySu\n",
    "        for Wm in [100,300,3000]:\n",
    "            print '\\t' + '\\t' + str(Wm)\n",
    "            W = [0,Wm]\n",
    "            f,ax = plt.subplots(1,4,figsize=(20,5))\n",
    "            ax[0].imshow(np.log10(Data[keySp][keySu])[W[0]:W[1],W[0]:W[1]],cmap='jet')\n",
    "            ax[0].set_title('Data Subsampled')\n",
    "            ax[1].imshow(np.log10(DataBoosted[keySp][keySu])[W[0]:W[1],W[0]:W[1]],cmap='jet')\n",
    "            ax[1].set_title('Data Enhanced with HiCPlus')\n",
    "            ax[2].imshow(np.log10(DataBoostHiC[keySp][keySu])[W[0]:W[1],W[0]:W[1]],cmap='jet')\n",
    "            ax[2].set_title('Data Enhanced with BoostHiC')\n",
    "            ax[3].imshow(np.log10(Data[keySp]['100'])[W[0]:W[1],W[0]:W[1]],cmap='jet')\n",
    "            ax[3].set_title('Original Data')\n",
    "            plt.savefig(figpath + 'CompareBoost_W'+ str(W[1]) + '_Spec' + keySp + '_Res' + keySu + '.png',dpi=300,format='png')\n",
    "            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESC\n",
      "\t0.1\n",
      "\t1.0\n",
      "\t10.0\n",
      "\t100.0\n",
      "CN\n",
      "\t0.1\n",
      "\t1.0\n",
      "\t10.0\n",
      "\t100.0\n"
     ]
    }
   ],
   "source": [
    "for keySp in KeysSpec:\n",
    "    path = data_path[keySp] + '/HiCPlus/'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    print keySp\n",
    "    for keySu in KeysSub:\n",
    "        print '\\t' + keySu\n",
    "        f = h5py.File(path + 'chr16_basic_' + keySu + '_HiCPlus.hdf5','w')\n",
    "        f['data'] = DataBoosted[keySp][keySu]\n",
    "        f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(197479, 1, 40, 40)\n"
     ]
    }
   ],
   "source": [
    "HiCsample = Data[keySp][keySu]\n",
    "## Subdivide the HiC matrix in subimages\n",
    "low_resolution_samples, index = divide(HiCsample)\n",
    "\n",
    "low_resolution_samples = np.minimum(HiC_max_value, low_resolution_samples)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "## Reshape the high-quality Hi-C sample as the target value of the training.\n",
    "sample_size = low_resolution_samples.shape[-1]\n",
    "padding = conv2d1_filters_size + conv2d2_filters_size + conv2d3_filters_size - 3\n",
    "half_padding = padding / 2\n",
    "output_length = sample_size - padding\n",
    "\n",
    "print low_resolution_samples.shape\n",
    "\n",
    "## Data Loader\n",
    "lowres_set = data.TensorDataset(torch.from_numpy(low_resolution_samples), torch.from_numpy(np.zeros(low_resolution_samples.shape[0])))\n",
    "lowres_loader = torch.utils.data.DataLoader(lowres_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "hires_loader = lowres_loader\n",
    "\n",
    "## Get Model\n",
    "model = models.Net(40, 28)\n",
    "model.load_state_dict(torch.load('model/pytorch_model_12000'))\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "_loss = nn.MSELoss()\n",
    "\n",
    "## Make predictions\n",
    "running_loss = 0.0\n",
    "running_loss_validate = 0.0 # WARNING : NOT USED\n",
    "reg_loss = 0.0 # WARNING : NOT USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ll = []\n",
    "for i,l in enumerate(lowres_loader):\n",
    "    l = Variable(l[0]).float()\n",
    "    l = l.cuda()\n",
    "    l = l.cpu()\n",
    "    ll.append(l)\n",
    "    \n",
    "    if i==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = torch.cat(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "   4   2   1  ...    0   0   0\n",
       "   2   0   2  ...    0   0   0\n",
       "   1   2   2  ...    0   0   0\n",
       "     ...       â‹±       ...    \n",
       "   0   0   0  ...    4   2   3\n",
       "   0   0   0  ...    2   0   2\n",
       "   0   0   0  ...    3   2   0\n",
       "[torch.FloatTensor of size 1x40x40]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
